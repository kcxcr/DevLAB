{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle: (https://www.kaggle.com/datasets/ramisashararnidhi/emotion-dataset?select=Untitled+spreadsheet+-+emotion_dataset.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import librosa\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 500\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_encoder = LabelEncoder()\n",
    "class Emotion_Dataset(Dataset):\n",
    "    def __init__(self, transform, mode):\n",
    "        self.transform = transform\n",
    "        self.audio, self.text, self.emotion = [],[],[]\n",
    "        \n",
    "        model_id = \"distilbert-base-uncased\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        df = pd.read_csv('spreadsheet-emotion_dataset.csv')\n",
    "        self.audio = df['audio_file'].tolist()\n",
    "        self.text = df['transcribed_data'].tolist()\n",
    "        self.emotion = df['emotion'].tolist()\n",
    "        train_size = int(len(df)*0.8)\n",
    "        if mode == 'train':\n",
    "            self.audio = self.audio[:train_size]\n",
    "            self.text = self.text[:train_size]\n",
    "            self.emotion = self.emotion[:train_size]\n",
    "            self.emotion = emotion_encoder.fit_transform(self.emotion)\n",
    "        else:\n",
    "            self.audio = self.audio[train_size:]\n",
    "            self.text = self.text[train_size:]\n",
    "            self.emotion = self.emotion[train_size:]\n",
    "            self.emotion = emotion_encoder.transform(self.emotion)\n",
    "        print(self.tokenizer.vocab_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio, text, emotion = self.audio[idx], self.text[idx], self.emotion[idx]\n",
    "        \n",
    "        audio, _ = librosa.load('audio_file/audio_file/'+audio, sr=16000)\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=100, n_fft=400, hop_length=160)\n",
    "        mfcc = scale(mfcc, axis=1)\n",
    "        pad2d = lambda a, i: a[:, 0:i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0], i-a.shape[1]))))\n",
    "        mfcc = pad2d(mfcc, 120)\n",
    "        \n",
    "        text = self.tokenizer(text, padding=True, truncation=True, add_special_tokens=True)['input_ids']\n",
    "        text = text + [0]*(20-len(text))\n",
    "        \n",
    "        mfcc = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)\n",
    "        text = torch.tensor(text, dtype=torch.long)\n",
    "        \n",
    "        return (mfcc, text, emotion)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((256,256))])\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((256,256))])\n",
    "train_set = Emotion_Dataset(transform=train_transforms, mode='train')\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_set = Emotion_Dataset(transform=test_transforms, mode='test')\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCBlock(nn.Module):\n",
    "    def __init__(self, out_dims):\n",
    "        super(MFCCBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=1, out_features=64)\n",
    "        self.fc2 = nn.Linear(64, out_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # [B, 32, H, W]\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))  # [B, 64, H, W]\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))  # [B, 96, H, W]\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if isinstance(self.fc1, nn.Linear) and self.fc1.in_features == 1:\n",
    "            self.fc1 = nn.Linear(x.size(1), 64).to(x.device)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBlock(nn.Module):\n",
    "    def __init__(self, out_dims, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(TextBlock, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, out_dims)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        x = F.relu(self.fc1(outputs[:, -1]))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModal(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(MultiModal, self).__init__()\n",
    "        out_dims = 64\n",
    "        self.audiomodel = MFCCBlock(out_dims=64)\n",
    "        self.textmodel = TextBlock(out_dims=64, vocab_size=30522, embedding_dim=100, hidden_dim=1024)\n",
    "\n",
    "        self.fc1 = nn.Linear(out_dims*2, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, audio, text):\n",
    "        audio_fc = self.audiomodel(audio)\n",
    "        text_fc = self.textmodel(text)\n",
    "        \n",
    "        x = torch.cat([audio_fc, text_fc], axis=1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModal(num_classes=6).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss: 1.7912, Val Loss: 1.79123, Val Macro F1: 0.00000\n",
      "Epoch [10], Train Loss: 1.7702, Val Loss: 1.77021, Val Macro F1: 0.08000\n",
      "Epoch [20], Train Loss: 1.7529, Val Loss: 1.75294, Val Macro F1: 0.08000\n",
      "Epoch [30], Train Loss: 1.7347, Val Loss: 1.73469, Val Macro F1: 0.08000\n",
      "Epoch [40], Train Loss: 1.6654, Val Loss: 1.66544, Val Macro F1: 0.08571\n",
      "Epoch [50], Train Loss: 1.6447, Val Loss: 1.64467, Val Macro F1: 0.22564\n",
      "Epoch [60], Train Loss: 1.7459, Val Loss: 1.74592, Val Macro F1: 0.22564\n",
      "Epoch [70], Train Loss: 1.7833, Val Loss: 1.78330, Val Macro F1: 0.35778\n",
      "Epoch [80], Train Loss: 2.0668, Val Loss: 2.06675, Val Macro F1: 0.15741\n",
      "Epoch [90], Train Loss: 2.1669, Val Loss: 2.16693, Val Macro F1: 0.18095\n",
      "Epoch [100], Train Loss: 2.4606, Val Loss: 2.46062, Val Macro F1: 0.29206\n",
      "Epoch [110], Train Loss: 2.5725, Val Loss: 2.57247, Val Macro F1: 0.32778\n",
      "Epoch [120], Train Loss: 2.8010, Val Loss: 2.80103, Val Macro F1: 0.29206\n",
      "Epoch [130], Train Loss: 1.8377, Val Loss: 1.83769, Val Macro F1: 0.26111\n",
      "Epoch [140], Train Loss: 2.4350, Val Loss: 2.43503, Val Macro F1: 0.57778\n",
      "Epoch [150], Train Loss: 2.5735, Val Loss: 2.57347, Val Macro F1: 0.43889\n",
      "Epoch [160], Train Loss: 2.7316, Val Loss: 2.73164, Val Macro F1: 0.43889\n",
      "Epoch [170], Train Loss: 2.8698, Val Loss: 2.86981, Val Macro F1: 0.43889\n",
      "Epoch [180], Train Loss: 2.9731, Val Loss: 2.97310, Val Macro F1: 0.43889\n",
      "Epoch [190], Train Loss: 3.0598, Val Loss: 3.05977, Val Macro F1: 0.43889\n",
      "Epoch [200], Train Loss: 3.1557, Val Loss: 3.15573, Val Macro F1: 0.43889\n",
      "Epoch [210], Train Loss: 3.2361, Val Loss: 3.23610, Val Macro F1: 0.43889\n",
      "Epoch [220], Train Loss: 3.2958, Val Loss: 3.29579, Val Macro F1: 0.43889\n",
      "Epoch [230], Train Loss: 3.3452, Val Loss: 3.34521, Val Macro F1: 0.57778\n",
      "Epoch [240], Train Loss: 3.4020, Val Loss: 3.40198, Val Macro F1: 0.57778\n",
      "Epoch [250], Train Loss: 3.4448, Val Loss: 3.44476, Val Macro F1: 0.57778\n",
      "Epoch [260], Train Loss: 3.4945, Val Loss: 3.49450, Val Macro F1: 0.57778\n",
      "Epoch [270], Train Loss: 3.5355, Val Loss: 3.53547, Val Macro F1: 0.57778\n",
      "Epoch [280], Train Loss: 3.5778, Val Loss: 3.57783, Val Macro F1: 0.57778\n",
      "Epoch [290], Train Loss: 3.6101, Val Loss: 3.61014, Val Macro F1: 0.57778\n",
      "Epoch [300], Train Loss: 3.6447, Val Loss: 3.64466, Val Macro F1: 0.57778\n",
      "Epoch [310], Train Loss: 3.6808, Val Loss: 3.68085, Val Macro F1: 0.57778\n",
      "Epoch [320], Train Loss: 3.7144, Val Loss: 3.71437, Val Macro F1: 0.57778\n",
      "Epoch [330], Train Loss: 3.7429, Val Loss: 3.74286, Val Macro F1: 0.57778\n",
      "Epoch [340], Train Loss: 3.7694, Val Loss: 3.76937, Val Macro F1: 0.57778\n",
      "Epoch [350], Train Loss: 3.7963, Val Loss: 3.79626, Val Macro F1: 0.57778\n",
      "Epoch [360], Train Loss: 3.8153, Val Loss: 3.81533, Val Macro F1: 0.57778\n",
      "Epoch [370], Train Loss: 3.8408, Val Loss: 3.84079, Val Macro F1: 0.57778\n",
      "Epoch [380], Train Loss: 3.8677, Val Loss: 3.86768, Val Macro F1: 0.57778\n",
      "Epoch [390], Train Loss: 3.8900, Val Loss: 3.89004, Val Macro F1: 0.57778\n",
      "Epoch [400], Train Loss: 3.9079, Val Loss: 3.90786, Val Macro F1: 0.57778\n",
      "Epoch [410], Train Loss: 3.9267, Val Loss: 3.92674, Val Macro F1: 0.57778\n",
      "Epoch [420], Train Loss: 3.9487, Val Loss: 3.94874, Val Macro F1: 0.57778\n",
      "Epoch [430], Train Loss: 3.9683, Val Loss: 3.96831, Val Macro F1: 0.57778\n",
      "Epoch [440], Train Loss: 3.9842, Val Loss: 3.98425, Val Macro F1: 0.57778\n",
      "Epoch [450], Train Loss: 4.0074, Val Loss: 4.00740, Val Macro F1: 0.57778\n",
      "Epoch [460], Train Loss: 4.0242, Val Loss: 4.02421, Val Macro F1: 0.57778\n",
      "Epoch [470], Train Loss: 4.0405, Val Loss: 4.04049, Val Macro F1: 0.57778\n",
      "Epoch [480], Train Loss: 4.0579, Val Loss: 4.05795, Val Macro F1: 0.57778\n",
      "Epoch [490], Train Loss: 4.0748, Val Loss: 4.07483, Val Macro F1: 0.57778\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for i, (audios, texts, emotions) in enumerate(train_loader):\n",
    "        audios = audios.float().to(device)\n",
    "        texts = texts.long().to(device)\n",
    "        emotions = emotions.long().to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(audios, texts)\n",
    "        loss = criterion(outputs, emotions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%10==0:\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        preds, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for (audios, texts, emotions) in test_loader:\n",
    "                audios = audios.float().to(device)\n",
    "                texts = texts.long().to(device)\n",
    "                emotions = emotions.long().to(device)\n",
    "                \n",
    "                pred = model(audios, texts)\n",
    "                \n",
    "                loss = criterion(pred, emotions)\n",
    "                \n",
    "                preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "                true_labels += emotions.detach().cpu().numpy().tolist()\n",
    "                \n",
    "                val_loss.append(loss.item())\n",
    "            \n",
    "            _val_loss = np.mean(val_loss)\n",
    "            _val_score = f1_score(true_labels, preds, average='macro')\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss: {loss.item():.4f}, Val Loss: {_val_loss:.5f}, Val Macro F1: {_val_score:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medvil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
