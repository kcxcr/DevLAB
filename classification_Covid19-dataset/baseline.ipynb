{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle dataset: (https://www.kaggle.com/datasets/pranavraikokte/covid19-image-dataset/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':50,\n",
    "    'LEARNING_RATE':0.0001,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVID_Dataset(Dataset):\n",
    "    def __init__(self, transform, mode):\n",
    "        self.transform = transform\n",
    "        self.image_folder = []\n",
    "        self.labels = []\n",
    "        categories = ['Covid', 'Normal', 'Viral Pneumonia']\n",
    "        \n",
    "        for i, category in enumerate(categories):\n",
    "            image_files = sorted([mode+'/'+category+'/'+f for f in os.listdir(mode+'/'+category) if \"mask\" not in f.lower()])\n",
    "            self.image_folder.extend(image_files)\n",
    "            self.labels.extend([i]*len(image_files))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_folder[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        return (img, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((CFG['IMG_SIZE'],CFG['IMG_SIZE']))])\n",
    "test_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Resize((CFG['IMG_SIZE'],CFG['IMG_SIZE']))])\n",
    "train_set = COVID_Dataset(transform=train_transforms, mode='train')\n",
    "train_loader = DataLoader(train_set, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "test_set = COVID_Dataset(transform=test_transforms, mode='test')\n",
    "test_loader = DataLoader(test_set, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trastion_module(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(Trastion_module, self).__init__()\n",
    "        # Composition Function을 적용하여 BN -> AF -> Conv순\n",
    "        self.tr_layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_ch, in_ch//2, kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "        # Feature Map 크기 감소\n",
    "        self.ave_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tr_layer(x)\n",
    "        x = self.ave_pool(x)\n",
    "        return x\n",
    "    \n",
    "class Botteleneck(nn.Module):\n",
    "    # Dense block의 레이어들이 출력하는 FeatureMap 크기: K = Growh_rate\n",
    "    def __init__(self, in_ch, growth_rate): \n",
    "        super(Botteleneck, self).__init__()\n",
    "\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_ch, out_channels=4*growth_rate, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.conv3x3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(4*growth_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=4*growth_rate, out_channels=growth_rate, \n",
    "                      kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identy = x\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.conv3x3(x)\n",
    "\n",
    "        x = torch.cat([identy, x], dim=1)\n",
    "        return x\n",
    "    \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_block, in_ch, growth_rate, last_stage=False):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.dense_ch = in_ch\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_block):\n",
    "            layer = Botteleneck(self.dense_ch, growth_rate)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "            self.dense_ch += growth_rate\n",
    "\n",
    "        if last_stage:\n",
    "            self.layers.append(nn.BatchNorm2d(self.dense_ch))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        else:\n",
    "            self.layers.append(Trastion_module(self.dense_ch))\n",
    "            assert self.dense_ch % 2 == 0,\n",
    "            self.dense_ch //= 2\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, block_list, growth_rate, n_classes=1000):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        # self.densenet121 = models.densenet121(pretrained=True)\n",
    "        # self.densenet121.classifier = nn.Linear(1024, 3)\n",
    "\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        self.stem = nn.Sequential( \n",
    "            nn.Conv2d(in_channels=3, out_channels=2*self.growth_rate,\n",
    "                      kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(2*self.growth_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.dense_ch = 2*self.growth_rate\n",
    "\n",
    "        dense_blocks = []\n",
    "        for i, num_block in enumerate(block_list):\n",
    "            last_stage = (i == len(block_list) - 1)\n",
    "            dense_blocks.append(DenseBlock(num_block, self.dense_ch, \n",
    "                                                    self.growth_rate, \n",
    "                                                    last_stage=last_stage))\n",
    "            self.dense_ch = dense_blocks[-1].dense_ch\n",
    "\n",
    "        self.dense_blocks = nn.Sequential(*dense_blocks)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.dense_ch, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = self.densenet121(x)\n",
    "        x = self.stem(x)\n",
    "        x = self.dense_blocks(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "model = DenseNet121(block_list=[6, 12, 24, 16], growth_rate=32, n_classes=3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss: 1.9267, Val Loss: 1.32337, Val Macro F1: 0.18841\n",
      "Epoch [1], Train Loss: 1.1848, Val Loss: 1.12428, Val Macro F1: 0.18841\n",
      "Epoch [2], Train Loss: 1.3356, Val Loss: 1.13511, Val Macro F1: 0.18841\n",
      "Epoch [3], Train Loss: 1.4236, Val Loss: 1.16623, Val Macro F1: 0.18841\n",
      "Epoch [4], Train Loss: 1.3569, Val Loss: 1.13443, Val Macro F1: 0.18841\n",
      "Epoch [5], Train Loss: 1.2202, Val Loss: 1.09524, Val Macro F1: 0.18841\n",
      "Epoch [6], Train Loss: 1.4316, Val Loss: 1.13269, Val Macro F1: 0.18841\n",
      "Epoch [7], Train Loss: 1.1848, Val Loss: 1.04774, Val Macro F1: 0.34459\n",
      "Epoch [8], Train Loss: 1.5493, Val Loss: 1.12690, Val Macro F1: 0.18841\n",
      "Epoch [9], Train Loss: 1.0594, Val Loss: 0.90672, Val Macro F1: 0.52849\n",
      "Epoch [10], Train Loss: 1.0922, Val Loss: 0.85435, Val Macro F1: 0.46881\n",
      "Epoch [11], Train Loss: 1.4911, Val Loss: 0.84082, Val Macro F1: 0.56566\n",
      "Epoch [12], Train Loss: 0.9155, Val Loss: 0.69016, Val Macro F1: 0.85919\n",
      "Epoch [13], Train Loss: 1.0677, Val Loss: 0.64358, Val Macro F1: 0.72963\n",
      "Epoch [14], Train Loss: 0.6669, Val Loss: 0.60642, Val Macro F1: 0.76072\n",
      "Epoch [15], Train Loss: 0.9330, Val Loss: 0.59559, Val Macro F1: 0.75432\n",
      "Epoch [16], Train Loss: 0.5557, Val Loss: 0.53335, Val Macro F1: 0.75748\n",
      "Epoch [17], Train Loss: 0.3876, Val Loss: 0.44246, Val Macro F1: 0.76359\n",
      "Epoch [18], Train Loss: 1.0172, Val Loss: 0.46630, Val Macro F1: 0.88081\n",
      "Epoch [19], Train Loss: 0.9265, Val Loss: 0.44912, Val Macro F1: 0.87035\n",
      "Epoch [20], Train Loss: 0.4113, Val Loss: 0.32141, Val Macro F1: 0.88718\n",
      "Epoch [21], Train Loss: 0.0140, Val Loss: 0.74811, Val Macro F1: 0.58201\n",
      "Epoch [22], Train Loss: 0.4024, Val Loss: 0.37209, Val Macro F1: 0.85707\n",
      "Epoch [23], Train Loss: 0.1195, Val Loss: 0.29845, Val Macro F1: 0.82294\n",
      "Epoch [24], Train Loss: 0.9013, Val Loss: 0.41860, Val Macro F1: 0.89770\n",
      "Epoch [25], Train Loss: 0.4001, Val Loss: 0.29809, Val Macro F1: 0.87317\n",
      "Epoch [26], Train Loss: 0.0735, Val Loss: 0.32675, Val Macro F1: 0.78971\n",
      "Epoch [27], Train Loss: 0.1177, Val Loss: 0.32321, Val Macro F1: 0.78867\n",
      "Epoch [28], Train Loss: 0.3047, Val Loss: 0.36223, Val Macro F1: 0.89083\n",
      "Epoch [29], Train Loss: 0.2640, Val Loss: 0.26332, Val Macro F1: 0.89083\n",
      "Epoch [30], Train Loss: 0.1730, Val Loss: 0.26325, Val Macro F1: 0.90571\n",
      "Epoch [31], Train Loss: 0.1908, Val Loss: 0.22917, Val Macro F1: 0.87343\n",
      "Epoch [32], Train Loss: 0.1535, Val Loss: 0.24985, Val Macro F1: 0.83362\n",
      "Epoch [33], Train Loss: 0.1808, Val Loss: 0.29339, Val Macro F1: 0.81515\n",
      "Epoch [34], Train Loss: 0.1204, Val Loss: 0.33507, Val Macro F1: 0.80529\n",
      "Epoch [35], Train Loss: 0.1580, Val Loss: 0.22058, Val Macro F1: 0.85707\n",
      "Epoch [36], Train Loss: 0.2688, Val Loss: 0.24219, Val Macro F1: 0.90655\n",
      "Epoch [37], Train Loss: 0.4529, Val Loss: 0.27554, Val Macro F1: 0.91692\n",
      "Epoch [38], Train Loss: 0.7525, Val Loss: 0.36917, Val Macro F1: 0.88858\n",
      "Epoch [39], Train Loss: 0.3600, Val Loss: 0.31507, Val Macro F1: 0.90734\n",
      "Epoch [40], Train Loss: 0.2136, Val Loss: 0.27553, Val Macro F1: 0.90385\n",
      "Epoch [41], Train Loss: 0.3194, Val Loss: 0.48790, Val Macro F1: 0.87677\n",
      "Epoch [42], Train Loss: 0.0891, Val Loss: 0.37364, Val Macro F1: 0.79150\n",
      "Epoch [43], Train Loss: 0.2539, Val Loss: 0.30142, Val Macro F1: 0.89083\n",
      "Epoch [44], Train Loss: 0.0740, Val Loss: 0.23216, Val Macro F1: 0.84374\n",
      "Epoch [45], Train Loss: 1.0740, Val Loss: 0.53668, Val Macro F1: 0.89125\n",
      "Epoch [46], Train Loss: 1.0687, Val Loss: 0.55451, Val Macro F1: 0.87511\n",
      "Epoch [47], Train Loss: 0.2788, Val Loss: 0.44136, Val Macro F1: 0.89140\n",
      "Epoch [48], Train Loss: 0.0258, Val Loss: 0.28042, Val Macro F1: 0.87578\n",
      "Epoch [49], Train Loss: 1.3004, Val Loss: 0.51350, Val Macro F1: 0.88929\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(CFG['EPOCHS']):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs = imgs.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # print(outputs.argmax(1))\n",
    "        # print(labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch%1==0:\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        preds, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in test_loader:\n",
    "                imgs = imgs.float().to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                pred = model(imgs)\n",
    "                \n",
    "                loss = criterion(pred, labels)\n",
    "                \n",
    "                preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "                true_labels += labels.detach().cpu().numpy().tolist()\n",
    "                \n",
    "                val_loss.append(loss.item())\n",
    "            \n",
    "            _val_loss = np.mean(val_loss)\n",
    "            _val_score = f1_score(true_labels, preds, average='macro')\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss: {loss.item():.4f}, Val Loss: {_val_loss:.5f}, Val Macro F1: {_val_score:.5f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medvil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
